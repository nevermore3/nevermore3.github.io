* 随机现象：在一定条件下，并不总是出现相同结果的现象称为随机现象

* 随机变量：表示随机现象各种结果的变量。 在不同的条件下由于偶然因素的影响，可能取各种不同的值，具有不确定性和随机性，但这些取值落在某个范围的概率是一定的。此种变量称为随机变量。例如某一段时间内公共汽车站等车乘客人数，某一时间段内电话交换台收到的呼叫次数等等。都是随机变量的实例。


一个随机试验的可能结果(基本事件)的全体组成一个基本空间Ω。随机变量X是定义在基本空间Ω上的取值为实数的函数，即基本空间Ω中每一个点，也就是每个基本事件都有实轴上的点与之对应。掷一颗骰子 ，它的所有可能结果是出现1点、2点、3点、4点、5点和6点 ，若定义X为掷一颗骰子时出现的点数，则X为一随机变量，出现1，2，3，4，5，6点时X分别取值1，2，3，4，5，6



### 信息熵

* 熵：表示随机变量的不确定性  


#### 信息量
信息量是对信息的度量，就和时间的度量是秒一样(比如一天时间有多少秒，一件事情有多少信息量)，当我们考虑一个离散的随机变量X的时候，当我们观察到这个变量的一个具体值的时候，我们接受到了多少信息呢？  

多少信息用信息量来衡量，**我们接收到的信息和具体发生的事件有关**  

信息的大小和随机事件的概率有关，越小的事情发生了则产生的信息量越大。例如国足夺得了世界杯冠军。 而越大概率的事情发生了则产生的信息量越小， 例如中国乒乓球队夺得了世界冠军(99%的概率发生啊，没什么信息量)。  

满足信息量函数的形式条件应该如下：

* 因此一个具体事件的信息量应该是随着其发生概率而递减的，并且不能为负。

* 如果我们有两个不想关的事件x和y，那么我们观察到的两个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和`h(x,y) = h(x) + h(y)`,由于x和y是两个不相关的事件，则满足`p(x,y) = p(x)*p(y)`

因此`h(x)`一定和`p(x)`的对数有关(因为只有对数形式的真数相乘之后，能够对应对数的相加)，因此信息量公式如下：  

	h(x) = -log p(x)

信息量度量的是一个具体事件发生了所带来的信息，而熵是在结果出来之前对可能产生的信息量的期望----考虑该随机变量的所有可能取值，所有可能发生事件所带来的信息量的期望：

	H(x) = -sum(p(x)log(p(x)))
 
在机器学习中熵是表示随机变量分布的混乱程度，分布越混乱，则熵越大。

* 熵只依赖与随机变量的分布，与随机变量的取值无关
* 定义0log0=0 因为可能出现某个取值概率为0的情况
* 熵越大，随机变量的不确定性就越大，分布越混乱，随机变量状态越多

#### 交叉熵

广泛应用在逻辑回归sigmoid和softmax函数中作为损失函数使用，其主要作用是度量两个概率分布间的差异性信息，例如：  
p对q的交叉熵表示： q分布的自信息对p分布的期望，公式如下：  
![](https://www.zhihu.com/equation?tex=%5C%5CH%28p%2Cq%29%3DE_%7Bx%5Csim+p%7D%5B-log%5Cspace+q%28x%29%5D%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dp%28x%29log%5Cspace+q%28x%29)
其中，p是真实样本分布，q是预测得到样本分布，在信息论中，其公式表示，如果用错误的编码方式q去编码真实分布p的事件，需要多少bit数。其中逻辑回归中的损失函数就是交叉熵(从另一个角度，可以通过最大似然估计推导出来)其定义式子如下：  
![](https://www.zhihu.com/equation?tex=%5C%5CJ%28%CE%B8%29%3D-%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28y_ilog%5Cspace+h_%CE%B8%28x_i%29%2B%281-y_i%29log%5Cspace+%281-h_%CE%B8%28x_i%29%29%29)  
其中，yi是第i个样本的真实标签， h是sigmoid预测输出值，J 是凸函数，可以得到全局最优解。


参考文献
[如何测量这个世界的混乱](https://zhuanlan.zhihu.com/p/33066289)
[熵的理解](https://zhuanlan.zhihu.com/p/30854084)
[机器学习中熵的解释](https://zhuanlan.zhihu.com/p/35423404)

