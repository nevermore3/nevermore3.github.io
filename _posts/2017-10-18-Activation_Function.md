# 激活函数

----------
#### 1. 什么是激活函数  
在神经元中，输入的inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。  
![](http://upload-images.jianshu.io/upload_images/1667471-6d3b43bce94b33de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
#### 2. 为什么要用激活函数  
如果不用激活函数，每一层的输出就是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。  
如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用的各个非线性模型中。  
  
#### 3. 都有哪些激活函数 

##### (1)sigmod激活函数(将一个实数映射到[0,1]范围内)

----------
公式如下：  
![](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D99/sign=a46bd6f1dd33c895a27e9472d01340df/0df3d7ca7bcb0a4659502a5f6f63f6246b60af62.jpg)  
其导数如下：  
![](https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D236/sign=375012cedfca7bcb797bc02c88086b3f/64380cd7912397dde41ab3095182b2b7d0a2875f.jpg)  
其图像如下：  
![](http://p.blog.csdn.net/images/p_blog_csdn_net/chl033/612813/o_SigmoidFunction_701_thumb.gif)

缺点：  
1、反向传播时候很容易出现梯度消失问题。  
2、函数输出并不是zero-centered（以0为中心的）  
3、幂运算相对比较耗时  
**为什么会出现梯度消失现象：**  
反向传播算法当中，要对激活函数进行求导运算，（优化神经网络的方法是Back propagation，即：先计算输出层对应的loss，然后将loss以导数的形式不断向上一层网络传递，修正相应的参数，达到降低loss的目的。）sigmoid的导数表达式为如上所示，sigmoid的原函数和导函数图形如下：  
![](http://upload-images.jianshu.io/upload_images/1667471-d2c5493f3380d6f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
由上图可知，当x数值在较大或者较小时，导数很快趋于0. 造成梯度消失。 在神经网络当中，使得参数无法被更新，网络无法优化。  
**输出不是zero-centered**  
因为sigmoid函数的输出恒大于0. 这会导致模型训练的收敛速度变慢，比如![](http://www.zhihu.com/equation?tex=%5Csigma%28%5Csum_i+w_i+x_i+%2B+b%29) ,如果所有的xi 都为正数或者负数，那么对wi进行求导 结果都是正数或者负数，这会导致如下红色箭头所表示的阶梯式更新，这并非一个好的优化路径。
![](https://pic4.zhimg.com/v2-d290a1c0a8a9378de6a66ec229b907ab_b.png)
##### (2) tanh激活函数

----------
公式如下：  
![](http://upload-images.jianshu.io/upload_images/1667471-805a73120a9e74ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
![](http://upload-images.jianshu.io/upload_images/1667471-d75d69fc01b55d2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
图像如下所示：  
![](https://pic2.zhimg.com/v2-a39596b282f6333bced6e7bfbfe04dcd_b.png)
它解决了zero-centered的输出问题。然后梯度消失的问题依然存在。



![](http://images.cnitblog.com/blog2015/678029/201504/241221240623467.png)

从数学角度上来看Sigmoid函数对中央区的**信号增益**较大， 对两侧的**信号增益**小，在信号的特征空间映射上，有很好的效果。从神经科学上来看，中央区酷似神经元的兴奋态，两侧酷似神经元的抑制态，因此在神经网络学习中，可以将重点特征推向中央区， 将非重点特征推向两侧区。


----------
#### （3）Relu激活函数(Rectified Linear Units)[http://shuokay.com/2016/10/01/why-relu-work/](http://shuokay.com/2016/10/01/why-relu-work/ "Relu")

----------
公式如下：f(x) = Max(0, x)   
其图像如下：  
![](https://pic3.zhimg.com/v2-5c97f377cdb5d1f0bc3faf23423c4952_b.png)
> 相对于传统的sigmoid函数， Relu有三个作用 1 防止梯度弥散 2、稀疏激活性 3、加快计算只需要判断输入是否大于0
    
这样一个激活函数对数据会产生什么样的影响呢？ 注意到$Wx+b = 0$ 是一个超平面，在这个超平面的两侧， $x$受到了不同的对待，$Wx+b < 0 $的部分，函数值直接被挤压至0，而$Wx+b>0 $的部分，函数值保持不变，化成图就是：  
![](https://camo.githubusercontent.com/8c471fd48ebef07dcaf3c8ec2dad0d55cc33b217/687474703a2f2f696d672e626c6f672e6373646e2e6e65742f32303135303331363030353034363230323f77617465726d61726b2f322f746578742f6148523063446f764c324a736232637559334e6b626935755a5851766147467763486c755a5746792f666f6e742f3561364c354c32542f666f6e7473697a652f3430302f66696c6c2f49304a42516b46434d413d3d2f646973736f6c76652f37302f677261766974792f536f75746845617374)  
下方的数据点被推向横轴，左侧的数据点被推向了纵轴，形成了两条数据非常密集的直线（高维上就是超平面）。注意上图中，两个$Wx+b$被画成了正交的，通常在一个训练好的神经网络模型中，它们并不是正交的。非正交的情况、高维的情况需要读者自行想象。上图说明，只有一个象限（抱歉我不知道高维空间里面这个词叫什么，还是就叫象限吧）的信息被保留了，而其他象限的信息被不同程度地压缩了，而且压缩幅度非常大，使其完全无法恢复。这其实是非常不合理的，也许这里仍旧有一些可以加以区分的信息被压缩没了，当然，这些信息在其他卷积核中也许会有所体现，但这样断绝一切可能性的做法并不可取，因此便有了Leaky ReLU的想法。

**缺点**  
训练的时候很脆弱，很容易就“die”了，例如一个非常大的梯度流过一个Relu神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远等于0.如果learning rate很大，那么很有可能网络当中40%的神经元都“dead”了。

#### 两者的区别

----------
首先看下Sigmoid和relu的曲线  
![](https://pic4.zhimg.com/50/v2-074734a139fa7905e57ba9665ce7799f_hd.png)
可以得到sigmoid的导数  
![](https://pic3.zhimg.com/50/v2-750b1dbba0d98ab00625cdb2127f089a_hd.jpg)
relu的导数  
![](https://pic2.zhimg.com/50/v2-98171b7bb24e9960fde6cd0af3b3ed25_hd.jpg)
结论就是sigmoid的导数只有在0附近的时候有比较好的激活性，在正负饱和区的梯度都接近于0，所以这会造成梯度弥散，而relu函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象。第二，relu函数在负半区的导数为0 ，所以一旦神经元激活值进入负半区，那么梯度就会为0，也就是说这个神经元不会经历训练，即所谓的稀疏性。第三，relu函数的导数计算更快，程序实现就是一个if-else语句，而sigmoid函数要进行浮点四则运算。综上，relu是一个非常优秀的激活函数

2001年，神经科学家Dayan、Abott从生物学角度，模拟出了神经元接受信号更精确的激活模型，如下所示：  
![](http://images.cnitblog.com/blog2015/678029/201504/241258145158893.png)
![](http://images.cnitblog.com/blog2015/678029/201504/241900156879853.png)
这个模型对比Sigmod主要有三点变化1、单侧抑制 2、相对宽阔的兴奋边界 3、稀疏激活性，
#### 生物神经的稀疏激活性  
在神经科学方面，除了新的激活频率函数之外，神经科学家还发现了神经元的稀疏激活性。  
还是2001年，Attwell等人基于大脑能量消耗的观察学习上，推测神经元编码工作方式具有稀疏性和分布性。  
2003年Lennie等人估测大脑同时被激活的神经元只有1~4%，进一步表明神经元工作的稀疏性。
从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征  
从这个角度来看，在经验规则的初始化W之后，传统的Sigmoid系函数同时近乎有一半的神经元被激活，这不符合神经科学的研究，而且会给深度网络训练带来巨大问题。  
Softplus照顾到了新模型的前两点，却没有稀疏激活性。因而，校正函数max(0,x)成了近似符合该模型的最大赢家。

#### 关于稀疏性的观点

----------
- 信息解离  

当前，深度学习一个明确的目标是从数据变量中解离出关键因子。原始数据（以自然数据为主）中通常缠绕着高度密集的特征。原因是这些特征向量是相互关联的，一个小小的关键因子可能牵扰着一堆特征，有点像蝴蝶效应，牵一发而动全身

- 线性可分

稀疏特征具有更大可能线性可分，或者对于非线性映射机制有更小的依赖，因为稀疏特征处于高纬的特征空间上(被自动映射了)从流形学习观点来看（参见降噪自动编码器），稀疏特征被移到了一个较为纯净的低维流形面上。线性可分性亦可参照天然稀疏的文本型数据，即便没有隐层结构，仍然可以被分离的很好。

- 稠密分布但是稀疏

稠密缠绕分布着的特征是信息最富集的特征，从潜在性角度，往往比局部少数点携带的特征成倍的有效。而稀疏特征，正是从稠密缠绕区解离出来的，潜在价值巨大。

#### 基于稀疏性的校正激活函数

----------
几十年的机器学习发展中，我们形成了这样一个概念：非线性激活函数要比线性激活函数更加先进，尤其是在布满Sigmoid函数的BP神经网络，布满径向基函数的SVM神经网络中，往往有这样的幻觉，非线性函数对非线性网络贡献巨大。综上所述，在深度学习模型中，使用简单、速度快的线性激活函数可能更为合适。  
![](http://images.cnitblog.com/blog2015/678029/201504/250236342652777.png)

**Vanishing Gradient Problem**  
使用线性激活函数的另外一个原因就是，减轻梯度法训练模型时的梯度消失问题  
看过BP推导的人都知道，误差从输出层反向传播算梯度时，在各层都要乘当前层的输入神经元值，激活函数的一阶导数。  
即**Grad=Error⋅Sigmoid′(x)⋅x** 使用双端饱和(即值域被限制)Sigmoid系函数会有两个问题：  
1、Sigmoid'(x)∈(0,1)  导数缩放  
2、x∈(0,1)或x∈(-1,1)  饱和值缩放  
这样，经过每一层时，Error都是成倍的衰减，一旦进行递推式的多层的反向传播，梯度就会不停的衰减，消失，使得网络学习变慢。而校正激活函数的梯度是1，且只有一端饱和，梯度很好的在反向传播中流动，训练速度得到了很大的提高。Softplus函数则稍微慢点，Softplus'(x)=Sigmoid(x)∈(0,1) ，但是也是单端饱和，因而速度仍然会比Sigmoid系函数快
